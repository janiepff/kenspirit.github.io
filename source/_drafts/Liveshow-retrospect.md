title: 一块听听开发回顾总结
tags:
---

## 产品设计

一块听听性质和知乎 Live 都属于语音直播，由主讲人单向发送语音，文字，图片消息演讲，听众通过文字提问。  

为了能够快速开发上线，并让主讲人和听众容易参与，分享和传播，我们一开始就决定了不做原生的 App，而是开发 H5 页面。听众通过微信打开链接，就能直接登录，浏览所有的直播，购买和进入收听。  

由于做页面交互是非常耗时间和容易出 bug，所以我们尽量精简我们的前端功能。一块听听只有 4 个主页面：

1. 直播列表  
2. 直播详情  
3. 门票详情和购买  
4. 直播收听  

这几个页面只有直播收听页有较为复杂的互动功能，其它页面都只是基本信息展示，和页面跳转。在直播收听页面里，我们只有播放和暂停语音，提问和打赏的功能。当然系统级别还必须有实时接收主讲人语音和其它听众提问、打赏信息的能力。  

## 系统和架构设计

功能基本确定后，我们来看一看架构设计是怎么跟进的。  

针对于要尽量精简功能，快速上线系统，并让系统尽可能承载更多同时在线人数的目标，我们的理念是尽量把服务的压力转移到专门的服务提供商来负责。  

### 主讲人直播

前面只说到听众部分的 H5 页面，那么主讲人怎么直播？为了减少前端页面和工作量，我们决定让主讲人对着我们的微信公众号来直播。这样我们就能够直接利用微信本身自带的语音录制，文字，图片上传等原生功能来实现主讲人直播的功能。  

当然，这个选择也有不好的地方。主讲人因为在微信公众号直播，听众在另一个独立的 H5 页面，他就没法很容易知道当前直播的状态。但是，为了节省一些开发时间和资源，我们最后还是选择先这么做。  

那我们怎么让主讲人了解当前的直播状态，并回答听众的提问呢？因为微信公众号回复消息有三种方式：自动回复，客服消息和模板消息。不同类型的消息，是有不同的数量限制的。我们算了一下限制数量和使用特点，最后决定只能使用自动回复，因为这样完全不消耗消息额度。所以，当主讲人每发一条消息到公众号，我们就会把当前直播室的在线人数，提问个数，打赏金额等自动回复给他。同时，消息里带上一个 H5 页面的链接，打开后会罗列所有的提问以备回答。我们尝试考虑是否可以通过微信公众号来让主讲人直接回复问题，但是没能想到一种比较好的交互方式。所以最后还是只能做一个 H5 页面。这个回答问题的音频录音功能本想用 H5 来实现，但是发现浏览器兼容性实在不太好。所以目前是利用微信的 JS SDK 来录音。  

所以，对于主讲人的消息处理，我们都是让微信先承担了这部分的数据处理。等微信通知我们的公众号后台时，我们先及时回复微信，然后异步从微信拉取语音，并上传至七牛永久存储起来，最后通过 socket 服务器发到听众的收听页面。所以听众拉取语音消息收听的时候，已经是七牛的 CDN 静态文件了。  

### 听众听直播

根据一块听听当前的功能，和本身语音直播系统的特点，主要的系统压力目前有两个：  

1. 因为我们的发言票是限量的，先到先得，所以抢票（类秒杀）时，对 HTTP server 的压力

2. 直播开始时间，瞬时进入直播对 HTTP server 和维持同时在线人数的 socket server 的压力

#### 发言票的模型设计

虽然我们的限量发言票并不能和淘宝等电商的量相比，但是其实原理也是一样。我们的系统也要避免有限的资源不被重复购买，超卖，以及能够及时响应用户的操作。  

以前做传统行业的 ERP 系统，商品数量都在商品详情里面。因为并发量小，用的是 Oracle，购买的时候只是简单让它自减，然后登记交易记录到另一个表，确保在一个 Transaction 里面一并提交就可以了。RDBMS 已经做了基本的数据一致性保障。  

但是，现在我们的并发量比以前大很多，而且用的是 MongoDB，处理手段就不太一样了。  首先，MongoDB 单一 Collection 的更新是原子性的，所以我们无法用类似的手段，把票的余量放直播数据的 Collection，然后在另外的 Collection 生成购票记录。

那我们怎么做呢？考虑到我们的发言票数量并不大，才几百上千，所以，我们在创建直播的时候，就直接生成专门的票据记录，预留给用户抢票的时候用。这些票据上的记录，没有用户信息的支付记录，便是可供抢票的数量。用户抢票的时候直接通过获取没有用户信息的票据记录，并尝试锁定和支付就可以了。但我们怎么确保发言票不被重复购买，和超卖呢？根据 MongoDB 关于 Concurrent Edit 和 Use Cases 的分析文档。我们可以在票据上面加上一个特殊的字段（文档里叫 `nonce`）。这个字段存放的是一个类似 UUID 的值。用户对记录操作的时候，必须先拿到这个值，把这个字段的旧数据作为查询的一部分，然后更新为新生成的 UUID。如果同时有多用户尝试对该记录进行操作，数据库的引擎会检测到冲突并确保最多只有一个操作能成功。详情可参考以下[链接][]。  

[链接]: https://docs.mongodb.com/ecosystem/use-cases/metadata-and-asset-management/#create-and-edit-content-nodes

#### 维持同时在线人数

socket server 的压力在于如何能同时维持尽量多的人数并且不丢失消息。我们的 socket server 是 node.js 的 socket.io 集群和 redis 集群。为了让它能承载更多的同时在线人数，它只承担消息转发，和接入认证功能，所有的业务逻辑都放在 HTTP server 上。  

对 socket server 压测的时候，我们用的是 socket-bench。一开始的时候，单台 1G CPU 和内存的机器，大概才支持 ？？ 的同时连接数，并且保持低的消息丢失率。单这个数字并不是很好，后面更换并使用了 socket.io 的另一个 C++ 引擎后才好一些。目前我们的同时在线人数还不多，这方面还在持续优化中。  

由于这部分不是我负责，更细节的地方就不太清楚了。  

#### 消息缓存设计

当听众瞬时进入直播的时候，会并发大量的消息拉取请求。如果所有的请求直接压到数据库，肯定会出问题。  

因为我们的直播消息都是文本，即使是图片和语音消息，存的数据也是外部 CDN 的链接。再加上一场一小时的直播，主讲人加上听众发言的消息量顶多两三千，消息量并不大。所以，我们的消息都直接缓存在 Redis 集群。每一个直播对应一个 List。所以，当新进入直播间的用户拉取消息的时候，直接拉的都是 Redis 里的数据，完全不走数据库。  

主讲人直播的消息，存库成功后，缓存到 Redis 的同时会发布一个接收消息的事件到 socket 通知听众。听众发言，先直接走 socket server 并广播，然后再由 socket server 通知 HTTP server 存库和缓存。  


## 产品和技术坑

### 产品交互

#### 主讲人自动回复

前面说到了，主讲人和听众界面是分开的，我们花了不少时间讨论和尝试应该怎么让主讲人更好了解直播情况。  

因为是利用自动回复消息的功能，一开始的时候，我们考虑每条消息必回是否会非常打搅主讲人的直播。所以，我们尝试定各种规则来限定自动回复的条件，比如这次发消息和上次发消息之间的新增提问数量达到某个阀值。但是，这样的规则不仅提升逻辑复杂性，加长消息处理时间，带来了 bug，延长测试时间。而且，后来经实际主讲人尝试直播后，觉得还是无法掌握直播间情况，最后还是觉得每条消息都自动回复更好。最后，我们还支持消息撤回，所以更需要这种自动回复的频率。  

#### 直播间消息交互改版

听众主要使用的直播间页面，经历了三次改版。  

第一次是主讲人消息和听众提问消息的上下分屏，中间分隔线要支持上下滑动。但是其实这样的界面操作上并不太友好，而且通过分隔线的上下滑动，不同手机的交互和性能对操作流畅性上影响还是挺大的。  

由于上面的原因，前端把听众提问消息抽出，作为叠加层展示。用户需要看的时候再点击按钮展开和收起这个叠加层。这个界面存在的问题的，叠加层的透明度不好选择，太透明会和主讲人消息重叠导致阅读困难。不透明也不好看。最大的问题是，叠加层会略小于整个直播间，还要支持上下滑动看消息，所以里面的文字要缩小，阅读更困难。  

最后的一次就把所有的消息都显示在同一个区域，只是按一个按钮切换两种模式：「只看主讲人消息」和「所有人消息」的模式。目前感觉还是这个模式最合适。  

如果我们在实现这个直播间交互功能前，多考虑不同方面和尝试操作体验，应该可以避免反复改动，节省更多时间。  

### 技术坑

#### 主讲人消息处理相应慢

前面的主讲人消息处理方面说到，现在我们是先及时回复微信，再异步处理的。

但是，一开始我们的实现是等语音消息完全从微信拉取完毕，等成功保存到七牛，才回复微信的服务器。所以，时不时由于网络问题，导致语音处理慢，超过 5 秒，微信客户端就回复主讲人说公众号暂时无法提供服务。  

这里有两种情况，一种是语音处理其实没有问题的，最终能显示到听众的直播间。另一种情况就是语音最后因为超时而真的处理失败。两种情况其实都给主讲人带来困惑，到底我的语音发送成功没有？因为他和听众不在一个画面。所以主讲人都要和我们另外微信确认到底消息成功发送没有。  

一开始实时处理语音，其实考虑了两个方面因素。第一方面是因为主讲人和听众在不同的画面，希望能通过完全处理后是否存在异常，来通知微信，准确告诉主讲人情况，可以确定重发语音还是继续。但是，没想到网络处理的延时，更加剧了主讲人的担忧。第二方面是怕异步处理导致广播的，和缓存到 Redis 的消息时间错乱。  

后来考虑到微信接收了主讲人的消息后，有效期为 3 天。并且主讲人正常直播的话，每条语音通常都大于 10 秒。那两次发送语音的时间，留给系统异步处理的空间还是足够的。所以，为了让主讲人减少顾虑，按他的演讲节奏来直播，我们换成一开始说的方式，及时返回，异步处理。同时我们还加入自动重试机制和后台 CMS 手动刷新，同步消息数据机制。确保最终的语音消息都能准确按主讲人发布时间展示。  

这个问题让我更深刻考虑系统设计和架构：哪些是最有可能出错和失败的点；有什么失败恢复机制；怎么利用各个平台的特点，综合系统业务功能要求做出平衡，同时兼并考虑用户体验。  

#### 跨号支付

这个可以说是，我们客户端开发上最始料不及的坑。  

系统刚上线，某位主讲人推广后，频繁接到反馈说不少人无法成功购买，我们都感到很纳闷。后来发现原来无法购买的情况，都是听众通过主讲人在他自己的微信订阅号文章上的原文链接，跳转到我们「一块听听」的直播购买页面后才出现的。这是很严重也是必须修复的问题，因为我们产品的定位就是希望能让主讲人和用户容易分享。对于一个有影响力的主讲人来说，公众号分享再也正常不过了，如果这样就无法购买，肯定是无法接受的。  

我们原来是用 Ping++ 实现支付功能。在用户不能支付时候，它的报错信息根本不是微信自身的禁止跨号支付的提示。这导致我们花了很长的时间定位问题。基本确定是跨号支付的问题后，群求统一 iPhone 和 Android 机的处理手法也花了不少时间。我们测试时发现用链接跳转的话，iPhone 手机偶尔不行，Android 机都不行。把链接转成二维码，让听众扫码跳转的话，iPhone 就不行，Android 就可以。很是头痛。  

后来网上查到一些建议是先尝试用微信支付，如果不行的话，再用二维码支付的方式让用户扫码支付。所以，我们只好撤了不用 Ping++，直接用微信 JS SDK 来实现支付，才解决了这个问题。  


## 项目计划和安排

不必做，但是耗费开发和测试精力的地方：

 - 讨论更改：即将结束，已经结束
 - 在线人数
 - 主讲人直播时回复逻辑
 - 更多直播，随机逻辑，直播排序问题

Oct 7 后正式开发
Oct 13 真正集成上微信才完成语音信息处理逻辑
Oct 20 前端做了主讲人回复消息功能
Oct 24 转上 production 环境
Nov 1  开始压测



收益展现（总？门票 + 打赏？）
哪里有可能出错，按照失败的情况来设计
